{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13994950,"sourceType":"datasetVersion","datasetId":8918925}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# imports, seed and config\nimport os\nimport glob\nimport math\nimport random\nfrom collections import defaultdict, Counter\nfrom typing import List, Tuple, Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n\nDATA=[\"/kaggle/input/names-dataset/names.txt\"]\nMAX_NGRAM = 3          # build unigram, bigram, trigram counts\nBLOCK_SIZE = 3         # context length fed to MLP (number of previous tokens)\nEMBED_DIM = 24         # embedding dim for tokens in MLP\nHIDDEN_DIM = 128       # hidden units in MLP\nBATCH_SIZE = 512\nEPOCHS = 12            # adjust for runtime / quality tradeoff\nLR = 1e-3\nALPHA_LAPLACE = 1.0    # Laplace smoothing alpha\nK_CONFIDENCE_K = 5.0   # interpolation hyperparameter\nMAX_NAME_LEN = 30\nPRINT_EVERY = 200\nSEED_FOR_SAMPLING = 2147483647","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:25:32.740410Z","iopub.execute_input":"2025-12-05T03:25:32.740755Z","iopub.status.idle":"2025-12-05T03:25:36.960432Z","shell.execute_reply.started":"2025-12-05T03:25:32.740727Z","shell.execute_reply":"2025-12-05T03:25:36.959684Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# dataset loading\ndef data_path(candidates):\n    for p in candidates:\n        if \"*\" in p:\n            found = glob.glob(p)\n            if found:\n                return found[0]\n        if os.path.exists(p):\n            return p\n    return None\n\nDATA_PATH = data_path(DATA)\nif DATA_PATH is None:\n    raise FileNotFoundError(\"names.txt not found. Upload it to working dir or adjust DATA.\")\nwith open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n    raw_words = [line.strip() for line in f if line.strip()]\nwords = [w.lower() for w in raw_words]\nprint(f\"Loaded {len(words)} names. Example: {words[:8]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:25:36.961785Z","iopub.execute_input":"2025-12-05T03:25:36.962196Z","iopub.status.idle":"2025-12-05T03:25:37.035491Z","shell.execute_reply.started":"2025-12-05T03:25:36.962172Z","shell.execute_reply":"2025-12-05T03:25:37.034708Z"}},"outputs":[{"name":"stdout","text":"Loaded 32033 names. Example: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# vocab\nchars = sorted(list(set(\"\".join(words))))\nchars = [c for c in chars if c.isalpha()]  # keep alphabetic only\nitos = {0: \".\"}  # dot token for start/end\nfor i, ch in enumerate(chars, start=1):\n    itos[i] = ch\nstoi = {s: i for i, s in itos.items()}\nV = len(itos)\nprint(\"Vocab size:\", V)\nprint(\"Some tokens:\", dict(list(itos.items())[:10]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:25:37.036552Z","iopub.execute_input":"2025-12-05T03:25:37.036886Z","iopub.status.idle":"2025-12-05T03:25:37.047184Z","shell.execute_reply.started":"2025-12-05T03:25:37.036864Z","shell.execute_reply":"2025-12-05T03:25:37.045826Z"}},"outputs":[{"name":"stdout","text":"Vocab size: 27\nSome tokens: {0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i'}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ngrams counters\ndef build_ngram_counters(corpus: List[str], max_n: int) -> Dict[int, Dict[Tuple[int, ...], Counter]]:\n    counters = {n: defaultdict(Counter) for n in range(1, max_n + 1)}\n    for w in corpus:\n        seq = [0] + [stoi[c] for c in w] + [0]\n        for n in range(1, max_n + 1):\n            ctx_len = n - 1\n            for i in range(len(seq) - ctx_len):\n                ctx = tuple(seq[i:i+ctx_len]) if ctx_len > 0 else tuple()\n                nxt = seq[i+ctx_len]\n                counters[n][ctx][nxt] += 1\n    return counters\n\nngram_counters = build_ngram_counters(words, MAX_NGRAM)\nfor n in range(1, MAX_NGRAM+1):\n    print(f\"Order {n} contexts: {len(ngram_counters[n])}\")\n\ndef ngram_probs_for_context(counters, n, context, alpha, vocab_size):\n    cnts = counters[n].get(context, None)\n    if cnts is None:\n        arr = np.ones(vocab_size, dtype=float) * alpha\n        arr = arr / arr.sum()\n        return arr\n    arr = np.array([cnts.get(i, 0) for i in range(vocab_size)], dtype=float)\n    arr += alpha\n    arr = arr / arr.sum()\n    return arr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:25:37.049369Z","iopub.execute_input":"2025-12-05T03:25:37.049685Z","iopub.status.idle":"2025-12-05T03:25:37.421293Z","shell.execute_reply.started":"2025-12-05T03:25:37.049661Z","shell.execute_reply":"2025-12-05T03:25:37.420337Z"}},"outputs":[{"name":"stdout","text":"Order 1 contexts: 1\nOrder 2 contexts: 27\nOrder 3 contexts: 601\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# building supervised dataset for multilevel perceptron\ndef build_supervised_dataset(corpus: List[str], block_size: int) -> Tuple[torch.Tensor, torch.Tensor]:\n    X, Y = [], []\n    for w in corpus:\n        context = [0] * block_size\n        for ch in w + \".\":\n            ix = stoi[ch]\n            X.append(list(context))\n            Y.append(ix)\n            context = context[1:] + [ix]\n    X = torch.tensor(X, dtype=torch.long)\n    Y = torch.tensor(Y, dtype=torch.long)\n    return X, Y\n\nrandom.shuffle(words)\nn = len(words)\nn1 = int(0.8 * n)\nn2 = int(0.9 * n)\nXtr, Ytr = build_supervised_dataset(words[:n1], BLOCK_SIZE)\nXdev, Ydev = build_supervised_dataset(words[n1:n2], BLOCK_SIZE)\nXte, Yte = build_supervised_dataset(words[n2:], BLOCK_SIZE)\nprint(\"Shapes Xtr, Xdev, Xte:\", Xtr.shape, Xdev.shape, Xte.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:25:37.422341Z","iopub.execute_input":"2025-12-05T03:25:37.422586Z","iopub.status.idle":"2025-12-05T03:25:37.866793Z","shell.execute_reply.started":"2025-12-05T03:25:37.422552Z","shell.execute_reply":"2025-12-05T03:25:37.865990Z"}},"outputs":[{"name":"stdout","text":"Shapes Xtr, Xdev, Xte: torch.Size([182625, 3]) torch.Size([22655, 3]) torch.Size([22866, 3])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# mlp and training loop\nclass SimpleMLP(nn.Module):\n    def __init__(self, vocab_size, embed_dim, block_size, hidden_dim):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.block_size = block_size\n        self.fc1 = nn.Linear(embed_dim * block_size, hidden_dim)\n        self.act = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n    def forward(self, x):\n        e = self.embed(x)               # (batch, block_size, embed_dim)\n        e = e.view(e.size(0), -1)       # (batch, block_size*embed_dim)\n        h = self.act(self.fc1(e))\n        logits = self.fc2(h)\n        return logits\n\nmodel = SimpleMLP(V, EMBED_DIM, BLOCK_SIZE, HIDDEN_DIM)\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\ncriterion = nn.CrossEntropyLoss()\n\n#training\ndef get_random_batch(X, Y, batch_size):\n    idx = torch.randint(0, X.shape[0], (batch_size,))\n    return X[idx], Y[idx]\n\ndef train(model, Xtr, Ytr, Xdev, Ydev, epochs=EPOCHS, batch_size=BATCH_SIZE):\n    model.train()\n    iters = max(1, Xtr.shape[0] // batch_size)\n    for epoch in range(epochs):\n        epoch_loss = 0.0\n        for it in range(iters):\n            xb, yb = get_random_batch(Xtr, Ytr, batch_size)\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            if (it + 1) % PRINT_EVERY == 0:\n                print(f\"Epoch {epoch+1} iter {it+1}/{iters} loss {loss.item():.4f}\")\n        avg_loss = epoch_loss / iters\n        model.eval()\n        with torch.no_grad():\n            dev_logits = model(Xdev)\n            dev_loss = criterion(dev_logits, Ydev).item()\n        model.train()\n        print(f\"Epoch {epoch+1}/{epochs}: train_loss={avg_loss:.4f}, dev_loss={dev_loss:.4f}\")\n    return model\n\nprint(\"Training MLP (this may take time)...\")\nmodel = train(model, Xtr, Ytr, Xdev, Ydev)\nprint(\"Training finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:25:37.867684Z","iopub.execute_input":"2025-12-05T03:25:37.867952Z","iopub.status.idle":"2025-12-05T03:25:50.839461Z","shell.execute_reply.started":"2025-12-05T03:25:37.867927Z","shell.execute_reply":"2025-12-05T03:25:50.838639Z"}},"outputs":[{"name":"stdout","text":"Training MLP (this may take time)...\nEpoch 1 iter 200/356 loss 2.2962\nEpoch 1/12: train_loss=2.4068, dev_loss=2.2645\nEpoch 2 iter 200/356 loss 2.2325\nEpoch 2/12: train_loss=2.2348, dev_loss=2.2137\nEpoch 3 iter 200/356 loss 2.1425\nEpoch 3/12: train_loss=2.1932, dev_loss=2.1954\nEpoch 4 iter 200/356 loss 2.1737\nEpoch 4/12: train_loss=2.1730, dev_loss=2.1752\nEpoch 5 iter 200/356 loss 2.2093\nEpoch 5/12: train_loss=2.1602, dev_loss=2.1675\nEpoch 6 iter 200/356 loss 2.0988\nEpoch 6/12: train_loss=2.1474, dev_loss=2.1597\nEpoch 7 iter 200/356 loss 2.1127\nEpoch 7/12: train_loss=2.1361, dev_loss=2.1536\nEpoch 8 iter 200/356 loss 2.1917\nEpoch 8/12: train_loss=2.1315, dev_loss=2.1505\nEpoch 9 iter 200/356 loss 2.1122\nEpoch 9/12: train_loss=2.1236, dev_loss=2.1427\nEpoch 10 iter 200/356 loss 2.1009\nEpoch 10/12: train_loss=2.1125, dev_loss=2.1425\nEpoch 11 iter 200/356 loss 2.0323\nEpoch 11/12: train_loss=2.1120, dev_loss=2.1388\nEpoch 12 iter 200/356 loss 2.0602\nEpoch 12/12: train_loss=2.1080, dev_loss=2.1343\nTraining finished.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# combining ngram and mlp distributions\ndef get_best_ngram_order_and_context(context_tokens: List[int], counters):\n    for order in range(MAX_NGRAM, 0, -1):\n        ctx_len = order - 1\n        if ctx_len == 0:\n            ctx = tuple()\n        else:\n            if len(context_tokens) < ctx_len:\n                continue\n            ctx = tuple(context_tokens[-ctx_len:])\n        cnt_map = counters[order].get(ctx, None)\n        if cnt_map:\n            total = sum(cnt_map.values())\n            return order, ctx, total\n    return 1, tuple(), sum(ngram_counters[1].get(tuple(), Counter()).values())\n\ndef compute_combined_distribution(context_tokens: List[int]):\n    order, ctx, ctx_count = get_best_ngram_order_and_context(context_tokens, ngram_counters)\n    P_ngram = ngram_probs_for_context(ngram_counters, order, ctx, ALPHA_LAPLACE, V)\n    # Prepare block_size context for MLP input (pad left with zeros if necessary)\n    ctx_for_mlp = [0] * max(0, BLOCK_SIZE - len(context_tokens)) + context_tokens[-BLOCK_SIZE:]\n    x = torch.tensor([ctx_for_mlp], dtype=torch.long)\n    model.eval()\n    with torch.no_grad():\n        logits = model(x).squeeze(0)\n        probs_mlp = F.softmax(logits, dim=0).cpu().numpy()\n    weight = ctx_count / (ctx_count + K_CONFIDENCE_K)\n    P_final = weight * P_ngram + (1.0 - weight) * probs_mlp\n    P_final = np.maximum(P_final, 1e-12)\n    P_final = P_final / P_final.sum()\n    return P_final, order, ctx, ctx_count, weight","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:25:50.841254Z","iopub.execute_input":"2025-12-05T03:25:50.841836Z","iopub.status.idle":"2025-12-05T03:25:50.850962Z","shell.execute_reply.started":"2025-12-05T03:25:50.841813Z","shell.execute_reply":"2025-12-05T03:25:50.849798Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# sampling utilities\n\ndef sample_name(prefix: str = \"\", max_len: int = MAX_NAME_LEN, seed: int = None,\n                temperature: float = 1.0, top_k: int = None):\n\n    if seed is not None:\n        rng = np.random.default_rng(seed)\n    else:\n        rng = np.random.default_rng()\n\n    prefix_chars = [c for c in prefix.lower() if c.isalpha()]\n    for c in prefix_chars:\n        if c not in stoi:\n            raise ValueError(f\"Character '{c}' not in vocabulary\")\n\n    seq = [0] + [stoi[c] for c in prefix_chars]  # include starting dot then prefix tokens\n    generated = []\n    for _ in range(max_len):\n        P_final, order, ctx, cnt, weight = compute_combined_distribution(seq)\n        # apply temperature\n        if temperature != 1.0:\n            logits = np.log(P_final + 1e-20) / temperature\n            probs = np.exp(logits - np.max(logits))\n            probs = probs / probs.sum()\n        else:\n            probs = P_final\n        # top-k filter\n        if top_k is not None and 0 < top_k < V:\n            top_idxs = np.argpartition(-probs, min(top_k, V)-1)[:min(top_k, V)]\n            mask = np.zeros_like(probs, dtype=bool)\n            mask[top_idxs] = True\n            filtered = probs * mask\n            if filtered.sum() <= 0:\n                filtered = probs\n            probs = filtered / filtered.sum()\n        nxt = rng.choice(np.arange(V), p=probs)\n        if nxt == 0:\n            break\n        generated.append(itos[nxt])\n        seq.append(nxt)\n        if len(generated) >= max_len:\n            break\n\n    prefix_str = \"\".join(prefix_chars)\n    final = (prefix_str + \"\".join(generated)).capitalize()\n    return final\n\ndef interactive_loop():\n    print(\"N-gram + MLP Name Generator (interactive). Do note that Temp scales randomness so low temp is mild+safe and high temp is creative+chaotic. top-k limits choices to top k characters\")\n    while True:\n        try:\n            prefix = input(\"Enter starting letters (leave empty for any): \").strip()\n        except Exception:\n            print(\"input() not available in this environment. Exiting interactive mode.\")\n            break\n        if prefix.lower() in (\"quit\", \"exit\"):\n            print(\"Exiting.\")\n            break\n\n        k_str = input(\"How many names to generate? [default 10]: \").strip()\n        try:\n            k = int(k_str) if k_str else 10\n            if k <= 0:\n                raise ValueError()\n        except:\n            k = 10\n\n        temp_str = input(\"Temperature (float, default 1.0): \").strip()\n        try:\n            temperature = float(temp_str) if temp_str else 1.0\n        except:\n            temperature = 1.0\n\n        topk_str = input(\"Top-k (int, optional): \").strip()\n        try:\n            top_k = int(topk_str) if topk_str else None\n        except:\n            top_k = None\n\n        seed_str = input(\"Seed (int, optional): \").strip()\n        try:\n            seed = int(seed_str) if seed_str else None\n        except:\n            seed = None\n\n        results = []\n        attempts = 0\n        while len(results) < k and attempts < 10 * k:\n            nm = sample_name(prefix=prefix, max_len=MAX_NAME_LEN, seed=(seed + attempts) if seed is not None else None,\n                             temperature=temperature, top_k=top_k)\n            if nm and nm not in results:\n                results.append(nm)\n            attempts += 1\n\n        print(f\"\\nGenerated {len(results)} names (prefix='{prefix}'):\")\n        for i, nm in enumerate(results, 1):\n            print(f\"{i:2d}. {nm}\")\n        print()\n\n        cont = input(\"Continue? (y/n) [default y]: \").strip().lower()\n        if cont in (\"n\", \"no\"):\n            print(\"Stopping interactive session.\")\n            break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:25:50.851940Z","iopub.execute_input":"2025-12-05T03:25:50.852245Z","iopub.status.idle":"2025-12-05T03:25:50.904798Z","shell.execute_reply.started":"2025-12-05T03:25:50.852217Z","shell.execute_reply":"2025-12-05T03:25:50.903882Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"interactive_loop()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T03:25:50.905819Z","iopub.execute_input":"2025-12-05T03:25:50.906109Z","iopub.status.idle":"2025-12-05T03:26:35.036416Z","shell.execute_reply.started":"2025-12-05T03:25:50.906088Z","shell.execute_reply":"2025-12-05T03:26:35.035569Z"}},"outputs":[{"name":"stdout","text":"N-gram + MLP Name Generator (interactive). Do note that Temp scales randomness so low temp is mild+safe and high temp is creative+chaotic. top-k limits choices to top k characters\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter starting letters (leave empty for any):  h\nHow many names to generate? [default 10]:  20\nTemperature (float, default 1.0):  0.6\nTop-k (int, optional):  \nSeed (int, optional):  \n"},{"name":"stdout","text":"\nGenerated 20 names (prefix='h'):\n 1. Hary\n 2. Hailia\n 3. Harsonn\n 4. Hitandrey\n 5. Haddie\n 6. Hawa\n 7. Ha\n 8. Hudie\n 9. Hamalin\n10. Hann\n11. Huna\n12. Helan\n13. Hanna\n14. Hema\n15. Hana\n16. Hari\n17. Hala\n18. Hays\n19. Hai\n20. Hanelyn\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Continue? (y/n) [default y]:  n\n"},{"name":"stdout","text":"Stopping interactive session.\n","output_type":"stream"}],"execution_count":9}]}